{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1LYRRmzExfd"
      },
      "source": [
        "# Building a tokenizer, block by block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix-GGHnQExfe"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vgcjy3omExff",
        "outputId": "3115e4ff-6155-4ed6-c830-3d0755ffa579",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.7.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QDZBgMJmExfg",
        "outputId": "e3689a5c-e1fb-4977-9740-04649dd325d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "676a0ebc7a3a4602857c4268a08e0d2a",
            "95105f57eefb4324a6d122d0bf79f83d",
            "27e24305542f4e90ae2adb1cd923f240",
            "b9c5b31b7d9945feb31d381672fa5966",
            "2e8b33f99ae14b16ad12f03710333d82",
            "cd776dff4f9b44cfb282b4f0ece3e1c2",
            "a624787e1dfd489199fbcff371d581a0",
            "0dc71106d0e440248b1855a38c192f43",
            "049118e46ea445789e91a3fd7ea30a12",
            "82a9ee40694f4189b24f5a0745646360",
            "8accc9a50684452381fa02bb860cd535",
            "d463724fc8294ea480663c8ae0507349",
            "78bf22393eae48d4afc845a69382ce6c",
            "8483cbaf850c4c4baccb3b98e78baf6d",
            "b5a5556432dd4575a0618809633cdb98",
            "a2f72976ecf8495c907b151d8faf75cd",
            "08a206cccffa4e169465d54ef180751f",
            "6deb0394a04b4b44b39bf54eca2eaae6",
            "d33863443688475f86403cbd07a1005f",
            "d29f7367e316498baed8a04713cf0bf4",
            "f2b8475ea4704ffda8d91bd29805a52d",
            "5c42e28b812a45309b2045ffb228e2a3",
            "7e518eb2bc8344b0aa9eb7c14a151149",
            "e94c2d25b7f9488bbc91ce843888f8b4",
            "25f3a2cca5cd43da9e9c052ae8f8414b",
            "767ee614e9264e7a89ba3d297736b2bf",
            "7ea652327cfe4474ad4ecf5220bedee2",
            "e40edc09f962485bbca7440e7bef6696",
            "e952d7dd243d4618bc6b8cab4834d376",
            "0959731ba13140598c222e895d0c0280",
            "c9f7b550ceda449592431741df3992b5",
            "024da73dbaa44fd5ad4d31e4bdebe0a5",
            "376ccab1f2e0454999cce4cd79d35fd3",
            "f6c5bf989cb445649829087af9ba4f8f",
            "0c2183b2c2764547be0e821b478a3f52",
            "184c8b2ff5544a41b8765df41de9f0f9",
            "02324fb8484b48c5899d73645769d862",
            "d492b487b1db492aa037d97e0a7215d4",
            "5a621fd1bf804ec990132afccafa7fc6",
            "3a48d7ed7d0247bdb463753c6237a822",
            "632be762af004d98a676ee312a60ec6d",
            "5575ceb2fa504a33b49e5dbb4f482f25",
            "88af558171974cf7917094e97e0d2984",
            "5bf0ace517d443668eef4fb050560472",
            "d6744f28b34b4249beffc5fb3609fa57",
            "d8c540e6f2d649e7936a235f40bc7300",
            "f55c60bc6cc4477ab55be5c621681edd",
            "9fc42ab25f874d11a9fdbb5115c281c7",
            "7c44d86f1999419ba067effb37e0f95b",
            "6f53f65cc43e4981abd782f7fb4f437e",
            "6197736fc80e4de8b4d9089fd50dbe15",
            "35a3669770344987ad3472cea30cfa91",
            "96034bb5d4dd47dfbc5eb1271b6bbeeb",
            "7b0f44d2ddac405b9f9624aee9af037f",
            "99f5ec9446f446febd99b4b809d39d68",
            "e79f77ae73374e9cba67c85358a15649",
            "b5dade6cdb9f4f54b10243ba66ab4df8",
            "0a51cfa576b0469d9e095da733b96627",
            "4e8ad72280b54102b43976cdfebd3314",
            "8191efa29ad94e33a818721232e15fbd",
            "48ef6d4bbf684f798328260aaa29cf89",
            "e7274b90679a4deeabc07fb1d29db758",
            "03d3365671404e20a8bfb6e58fac60a8",
            "3a3ea8666df14aa7a2881ee11e7ea899",
            "03ec4ac4eb1d43d79557fe1b27a64c75",
            "81a5be4bc12e4668ae9563858f1e5b3b",
            "3ed689dc49d046f5b28b7cc85c3ca891",
            "b858aa57399541edab79982fd0ebcfaa",
            "c7d5dfe2a12348608adb735f87949d16",
            "5d6fad4505e846999e526ecc295a62a8",
            "ba9d587557a34c828bcd1353270e6e2d",
            "c3820390ff44431fad62c489418bfdd9",
            "84b62f2c7bab443999bfb08efb350d88",
            "fb825293eca746e4a79a11c25a2721cc",
            "a71c893465d9401fbeb9278bc41ed198",
            "d9de7ee40cd741ae906a5795d75bf8f0",
            "a4e06af6b91240be9121c04fd341f522"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "676a0ebc7a3a4602857c4268a08e0d2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d463724fc8294ea480663c8ae0507349"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e518eb2bc8344b0aa9eb7c14a151149"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6c5bf989cb445649829087af9ba4f8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6744f28b34b4249beffc5fb3609fa57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e79f77ae73374e9cba67c85358a15649"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ed689dc49d046f5b28b7cc85c3ca891"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "\n",
        "def get_training_corpus():\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f_sY03McExfh"
      },
      "outputs": [],
      "source": [
        "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i in range(len(dataset)):\n",
        "        f.write(dataset[i][\"text\"] + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y3Sx49F2Exfh"
      },
      "outputs": [],
      "source": [
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Bc5c8BMMExfi"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QeHOISUXExfi"
      },
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8C2cy-iMExfj",
        "outputId": "ae858269-c902-4b10-c30c-f90755899c3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are u?\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "il95qlWMExfk"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g3SrCmSIExfk"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "v0L5Lw9vExfl",
        "outputId": "7ea1d629-5ca9-49ea-9ae4-9dbea294ccfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Let', (0, 3)),\n",
              " (\"'\", (3, 4)),\n",
              " ('s', (4, 5)),\n",
              " ('test', (6, 10)),\n",
              " ('my', (11, 13)),\n",
              " ('pre', (14, 17)),\n",
              " ('-', (17, 18)),\n",
              " ('tokenizer', (18, 27)),\n",
              " ('.', (27, 28))]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vJ1UbmJXExfl",
        "outputId": "7691a313-1d2d-40f9-c9a2-d7289df6882e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"Let's\", (0, 5)),\n",
              " ('test', (6, 10)),\n",
              " ('my', (11, 13)),\n",
              " ('pre-tokenizer.', (14, 28))]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
        "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FpIooOKgExfl",
        "outputId": "0e058740-1bb6-47c9-c16e-91c3c7db870a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Let', (0, 3)),\n",
              " (\"'\", (3, 4)),\n",
              " ('s', (4, 5)),\n",
              " ('test', (6, 10)),\n",
              " ('my', (11, 13)),\n",
              " ('pre', (14, 17)),\n",
              " ('-', (17, 18)),\n",
              " ('tokenizer', (18, 27)),\n",
              " ('.', (27, 28))]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence(\n",
        "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
        ")\n",
        "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ypvv5pY1Exfl"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qabBOcJjExfm"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nWEwCC4TExfm"
      },
      "outputs": [],
      "source": [
        "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "t-lnMDPxExfm",
        "outputId": "6cb7f240-184d-4b63-db11-c813e6b80ee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fpznerDfExfm",
        "outputId": "bbb5c1fc-016b-4489-ce81-271c689ce660",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 3\n"
          ]
        }
      ],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kLJEHgpqExfm"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JyN6hFCKExfm",
        "outputId": "14e72532-e5f8-4893-f92b-f458972a4b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mEADKU_xExfm",
        "outputId": "bc9e40cf-7ac2-45f4-cd02-71540c2c6f18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Q5UMJC0mExfp"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "H-kUElcPExfq",
        "outputId": "f5b04ef4-c40d-4af3-d4df-58e3ccb7b06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"let ' s test this tokenizer... on a pair of sentences.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jXTZQDTbExfq"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7zp6K3DgExfq"
      },
      "outputs": [],
      "source": [
        "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9ozRR63sExfq"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YiNpMhrlExfq"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Y3la-J-qExfq"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JVU7JwX6Exfq"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "whnr7mcyExfr",
        "outputId": "7a48d0f9-825f-4c9b-81b0-feb595ec19a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Let', (0, 3)),\n",
              " (\"'s\", (3, 5)),\n",
              " ('Ġtest', (5, 10)),\n",
              " ('Ġpre', (10, 14)),\n",
              " ('-', (14, 15)),\n",
              " ('tokenization', (15, 27)),\n",
              " ('!', (27, 28))]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OBRSGntWExfr"
      },
      "outputs": [],
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3BjjQyzrExfr"
      },
      "outputs": [],
      "source": [
        "tokenizer.model = models.BPE()\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "UvlRUL3zExfr",
        "outputId": "8b49834b-027e-4e58-fc3f-d7120fe32900",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['L', 'et', \"'\", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6l7jhpe1Exfr"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lD5iqBtcExfr",
        "outputId": "3f69c358-d596-4448-ebf8-dd78d4ad494a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' test'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "sentence = \"Let's test this tokenizer.\"\n",
        "encoding = tokenizer.encode(sentence)\n",
        "start, end = encoding.offsets[4]\n",
        "sentence[start:end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FLHM__q9Exfr"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.ByteLevel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "o_wzYwouExfs",
        "outputId": "98414885-5b43-46ac-8e06-f4277d46140a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Let's test this tokenizer.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "EN1elta7Exfs"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<|endoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "yEbsmXQNExfy"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PvDYBAbBExfz"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.Unigram())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2oZSBl5bExfz"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Regex\n",
        "\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [\n",
        "        normalizers.Replace(\"``\", '\"'),\n",
        "        normalizers.Replace(\"''\", '\"'),\n",
        "        normalizers.NFKD(),\n",
        "        normalizers.StripAccents(),\n",
        "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "BGlMOZzNExfz"
      },
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "wipK4mIiExfz",
        "outputId": "f8a142ff-89c3-4139-ead4-e9b2861fd055",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"▁Let's\", (0, 5)),\n",
              " ('▁test', (5, 10)),\n",
              " ('▁the', (10, 14)),\n",
              " ('▁pre-tokenizer!', (14, 29))]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "zORAl_-RExfz"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
        "trainer = trainers.UnigramTrainer(\n",
        "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
        ")\n",
        "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wkBF6pOpExfz"
      },
      "outputs": [],
      "source": [
        "tokenizer.model = models.Unigram()\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "I0A6KxroExfz",
        "outputId": "342b01e0-f42d-4f36-cd84-ecad5030ad63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "U-6oOppfExf0",
        "outputId": "d20d3861-320f-4510-99d2-ebb9f04b72b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1\n"
          ]
        }
      ],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
        "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "J1heAU-NExf0"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
        "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
        "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "pvPy4_V7Exf0",
        "outputId": "15a66b26-f203-483d-f39d-426197e3fd0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁Let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', '▁of', '▁sentence', 's', '!', '<sep>', '<cls>']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "57s737dDExf0"
      },
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.Metaspace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "SJcOKUntExf0"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    cls_token=\"<cls>\",\n",
        "    sep_token=\"<sep>\",\n",
        "    mask_token=\"<mask>\",\n",
        "    padding_side=\"left\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "lVbY6IknExf0"
      },
      "outputs": [],
      "source": [
        "from transformers import XLNetTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EGTJbJZdFGbW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Building a tokenizer, block by block",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
