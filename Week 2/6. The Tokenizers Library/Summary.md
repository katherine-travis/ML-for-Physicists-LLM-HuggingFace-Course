# Summary of Chapter 6: The Tokenizers Library

When training a model from scratch, using a tokenizer trained on a collection of data from a different domain or different language wiil yield poor results. This chapter focuses on how to train new tokenizers on a large, structured collection of real-world texts with the Tokenizers library that provides fast tokenizers for the Transformers library.

##Training a New Tokenizer from an Old One

Training a tokenizer involves a statistical process where the tokenixer examines all given texts in order to identify which subwords occur most frequently. The precise rules depend on the tokenization algorithm. This process is different from model training because it is deterministic rather than stochastic. Training the same alglorithm on the same data will produce the same results. This chapter demonstrates using the Datasets library and loading the CodeSearchNet dataset containing Python code from GitHub repositories. To handle large datasets efficiently, the approach uses Python generators rather than loading everything into memory, creating iterator objects that yield batches of texts only when needed. The train_new_from_iterator method trains new tokenizers with characteristics matching existing ones, which makes the process take only a few minutes, even on large datasets. Fast tokenizers are backed by the Hugging Face Tokenizers library written in Rust, while slow tokenizers are written purely in Python. Tokenizers can be saved using save_pretrained and uploaded to the Hub using push_to_hub for sharing with the community.

## Fast Tokenizers

Fast tokenizers provide special BatchEncoding objects with additional methods beyond simple dictionary functionality. Their key feature is offset mapping that tracks original text spans for final tokens. This offset tracking enables mapping words to generated tokens and mapping characters to containing tokens. Fast tokenizers can be identified through the is_fast attribute on both tokenizer and encoding objects. The BatchEncoding object provides methods like tokens() to access tokens without converting IDs, word_ids() to determine which word each token originates from, and word_to_chars() or token_to_chars() methods to map between tokens and character positions in original text. These capabilities are useful for token classification tasks like named entitiy recognition. Offset mapping enables property entity grouping without complex custom code for different tokenizer types, as character spans can be extracted directly from the original text using start and end positions from the first and last tokens of grouped entities.

## Byte-Pair Encoding Tokenization

Byte-Pair Encoding was developed as a text compression algorithm and used by OpenAI for tokenization when pretraining GPT. BPE training begins by computing the unique set of words in the data after normalization and pre-tokenization, then builds vocabulary from all symbols used to write those words. The base vocabulary contains all ASCII characters, with additional Unicode characters. The algorithm iteratively adds new tokens until reaching the desired vocabulary size by learning merge rules that combine two existing vocabulary elements into new ones. At each trainign step, BPE searches for the most frequent consecutive token pair in words, merges that pair, and repeats the process. GPT-2 tokenizers handle unknown characters by viewing words as bytes rather than Unicode characters, resulting in a small base vocabulary size of 256, while ensuring all characters can be inculded without conversion to unknown tokens. This is called byte-level BPE. The tokenization pipeline involves normalization, pre-tokenization, splitting words into individual characters, and applying learned merge rules in order.

## Building a Tokenizer

This section shows how to construct a tokenizer component by component, including normalization, pre-tokenization, the tokenization model itself, and post-processing. The Hugging Face Tokenizers library enables customization of each component independently. This allows tokenizers to be tailored to specific requirements and domains. 
