## Summary

Section 11 focuses on Supervised Fine-Tuning (SFT) of pretrained LMs using labeled instruction-response data, while using LoRA (Low-Rank Adapters) to make fine-tuning efficient and parameter-sparse. In practice you prepare a cleaned, tokenized dataset of prompts and targets, freeze most of the base model weights, and inject small low-rank adapter matrices into attention / feed-forward layers; only those adapter parameters are trained, which dramatically reduces GPU memory and storage needs. This workflow preserves the base modelâ€™s general knowledge, speeds up training, and makes sharing updates cheaper (you can share just the LoRA weights), but it still requires careful hyperparameter tuning, proper batching/prompt formatting, and evaluation on held-out instruction examples to avoid overfitting. Monitoring training/validation loss and task metrics shows how well SFT+LoRA is improving task performance; if validation loss plateaus or diverges from training loss you should reduce learning rate, add regularization, or check data quality. Overall, SFT+LoRA is a practical compromise for adapting large models to supervised tasks while keeping compute and storage costs manageable.
