# ML-for-Physicists-LLM-HuggingFace-Course
Understanding Natural Language Processing (NLP) and Large Language Models (LLMs) Using the Huggingface Course

I: Motivation and background:


For our final project we will follow the Hugging Face LLM Course, which offers a structured introduction to modern machine learning workflows using transformer-based models. We selected this project because both group members want a practical, guided path into deep learning that does not require extensive prior experience. Following a structured course allows us to build foundational knowledge of Large Language Models through conceptual learning and implementation. Completing this curriculum will give us practical experience with tokenization, transformer architecture, dataset handling and building reasoning models.

II: Project Goals: 

Our primary goal is to complete the entire Hugging Face LLM Course, including all hands on exercises, coding tasks, fine-tuning, and quizzes. We will document our progress in an organized GitHub repository that contains all notebooks and scripts from each module, any additional code used for fine-tuning, evaluation or demonstration, a summary of the key takeaways from each section, and a final report summarizing what we accomplished, what we learned, and what did or did not work.

III: Methodology and work plan

Setup:
Create GitHub repository with a clear directory structure
Setup CoLab configuration for GPU access
At the end of each week, we will upload all notebooks and scripts from each module completed that week as well as a summary of the key takeaways from those sections.
Week 1: Nov 17-23
Transformer Models
Using Transformers
Fine Tuning a Pre Trained Model
Sharing Models and Tokenizers
Week 2: Nov 24-30
The Datasets Library
The Tokenizers Library
Classical NLP Tasks
How to Ask for Help
Week 3: Dec 1-7
Building and Sharing Demos
Curate High Quality Datasets
Fine Tune Large Language Models
Build Reasoning Models
Week 4: Dec 8-14
Write final lab report

IV: Division of Responsibilities:

Both team members will independently complete every module to ensure that we both learn the full material and have the foundations required for each subsequent module. Organizational duties will be divided to keep the workload organized and fairly divided.

Katherine:
Responsible for organizing code for odd numbered chapters
Writes the lab notebook entries for those modules
Updates GitHub with documented files
Creates summaries for odd numbered chapters
Colette: 
Responsible for organizing code for even numbered chapters
Writes the lab notebook entries for those modules
Updates GitHub with documented files
Creates summaries for even numbered chapters

Responsibilities will alternate by chapter, but all learning and coding will be completed by both members. We will meet regularly to ensure consistency across the repository and final report.

