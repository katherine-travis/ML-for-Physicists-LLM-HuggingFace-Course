# Summary
## Transformer architectures
One of the main takeaways from this section on transformer models was learning which architecture is best for different tasks. The three architectures were encoder only, decoder only, and encoder-decoder. 

Encoder only models only use the encoder of the transformer. The attention layers access every word in the given sentence at each layer. By masking some of the words, the model has to figure out what the original sentence was. This type of architecture is best for when you need to understand an entire sentence. More specifically, it's best at sentence classification, name entity recognition, and extractive question answering. The BERT model and its variations are good examples of encoder only models. 

Decoder only models only use the decoder of the transformer. Instead of having access to every word like an encoder model, the decoder model only has access to prior word. This makes it useful for predicting the next word in a sentence. It's best for text generation, conversational AI, and creative writing. GPT and LLaMA are two prominent examples of decoder only models. 

Encoder-decoder models utilize both parts of the transformer. The encoder accesses the entire sentence and breaks down its meaning. The decoder uses that to generate an output sequence one step at a time. Using both the encoder and decoder together makes it useful at creating sentences based on a prompt, more specifically for summarization, translation, on generative question answering. 

## Bias
The other section that I found particularly interesting referred to the Bias_and_limitations_clean.ipynb code. I learned that it's beneficial to use a pretrained model because it reduces your carbon footprint, but the problem with this is that some of the models use "bad" data to train on, so the results may not be the best when others try to use that pretrained model. The example this notebook looked at was using a BERT based model to fill in the blank. They wanted it to find the top 5 most likely words that would finish the sentence. The sentences were "This man works as a ___" and "This woman works as a ___." The top 5 most likely results for what a mans profession would be were things like lawyer, carpenter, doctor, waiter, and mechanic, while the most likely results for a woman were nurse, waitress, teacher, maid, and prostitute. The data this model was trained on was supposedly neutral (they used the English Wikipedia and BookCorpus datasets), but it inevitably shows bias and generated some sexist responses. This example just goes to show that your results are only as good as the data you train on. If your training data is biased to be sexist, racist, or homophobic, your results will show that bias. They emphasized the importance of fine-tuning your pre-trained model to remove those biases while minimizing your carbon footprint.
